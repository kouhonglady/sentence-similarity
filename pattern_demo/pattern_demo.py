import re
import jieba as jb
import os
import shutil
import pickle
import pandas as pd
from sklearn.externals import joblib

"""
   rm the characters like ,/; except chinese and numbers . cut the sentences to words,and pair the to pattern
                each lines of file is contains two sentences,and a tag 0(means two sentences have different  meaning) or 1( same meaning). 
    setence_word_q1/setence_word_q2: use the Chinese Word Segmentation API jieba to cut the q1/q2 sentences to words,and save it in file  {setence_word_q1/setence_word_q2} 
    word_to_pattern : pair one word of q1 and one word of q2 to pattern,and save all the pattern in file{word_to_pattern}
"""

root_path = r'E:\study\hrg_project\environment\dataset\precision_data'

data_path = root_path + r'/original/train_weizhong.csv'
word_dict = root_path + r'/original/Chinese_stopwords.txt'

sentence_word_q1 = root_path + r'/dataset/train_15_word_q1.txt'
sentence_word_q2 = root_path + r'/dataset/train_15_word_q2.txt'
word_to_pattern = root_path + r'/dataset/word_to_pattern.txt'
final_pattern = root_path + r'/dataset/final_pattern.txt'
pairs_features = root_path + r'/dataset/pairs_features.txt'
result_path = root_path + r'/result.txt'
model_path = root_path + r'/model_weizhong.txt'
M = 1000

dataset = pd.read_csv(data_path)
data = dataset[0:M]
#data = dataset
Length, Width = data.shape


def step1_cutword_to_pattern():
    print("step1_cutword_to_pattern :start")

    #非中文和数字字符 \u0030-\u0039 数字0-9  \u4e00-\u9fa5 所有中文字符
    pattern = re.compile(r'[^\u4e00-\u9fa5]')

    #将所有的q1，去除非中文字符，并且进行结巴分词，之后一行一个句子的方式保存

    print("start cutting setence one")
    with open(sentence_word_q1, "w",encoding='utf-8') as f:
        i = 0;
        while i < Length:
            q1= re.sub(pattern, "",data['q1'][i])
            # print(q1)
            temp = jb.cut(q1, cut_all=False)
            f.write(" ".join(temp) + "\n")
            i += 1
        f.close()
    print("cutting setences one has finished")

    #将所有的q2，去除非中文字符，并且进行结巴分词，之后一行一个句子的方式保存
    print("start cutting setence two")
    with open(sentence_word_q2, "w",encoding='utf-8') as f:
        i = 0;
        while i < Length:
            q2= re.sub(pattern, "",data['q2'][i])
            # print(q2)
            temp = jb.cut(q2, cut_all=False)
            f.write(" ".join(temp)+ "\n")
            i += 1
        f.close()
    print("cutting setences two has finished")
    print("step1 end")

def rm_stopwords(file_path, word_dict):
    """
        rm stop word for {file_path}, stop words save in {word_dict} file.
        file_path: file path of file generated by function splitwords.
                    each lines of file is contains many words ,each word separate from each other by space.
        word_dict: file containing stop words, and every stop words in one line.
        output: file_path which have been removed stop words and overwrite original file.
    """

    # 从文件中将停用词读取出来，并保存到字典stop_dict中
    stop_dict = {}
    with open(word_dict,'r', encoding='UTF-8') as d:
        for word in d:
            stop_dict[word.strip("\n")] = 1
    # 如果存在临时文件将其删除
    if os.path.exists(file_path + ".tmp"):
        os.remove(file_path + ".tmp")

    print ("now remove stop words in %s." % file_path)
    # 安行删除停用词
    with open(file_path,'r', encoding='UTF-8') as f1, open(file_path + ".tmp", "w", encoding='UTF-8') as f2:
        for line in f1:
            tmp_list = []  # 将非停用词保存下来
            words = line.split()
            #	        print(line)
            for word in words:
                if word not in stop_dict:
                    tmp_list.append(word)
            words_without_stop = " ".join(tmp_list)
            to_write = words_without_stop + "\n"
            f2.write(to_write)
        f1.close()
        f2.close()
    # 将去除停用词的pattern文件重新到原路径
    shutil.move(file_path + ".tmp", file_path)
    print ("stop words in %s has been removed." % file_path)



def step2_cut_top_word():
    print ("step2_cut_top_word: now combine two words ,one from setence one and another from setence two. and save in %s." %word_to_pattern)
    #q1,q2 分词之后，分别两两组合，形成pattern
    with open(sentence_word_q1,"r",encoding ='utf-8')as f1,open(sentence_word_q2,'r',encoding = 'utf-8') as f2,open(word_to_pattern,'w',encoding='utf-8') as f3:
        line1 = f1.readline()
        line2 = f2.readline()

        while line1 and line2:
            temp=""
            for t in line1.split():
                for k in line2.split():
                    temp = temp+" "+t+"###"+k
            if temp is "":
                temp = "#:1"
            f3.write(temp+'\n')
            # print(temp)
            line1 = f1.readline()
            line2 = f2.readline()
        f1.close()
        f2.close()
        f3.close()
    # rm_stopwords(word_to_pattern, word_dict)
    print("step end. combine  words to pattern has  finished")


def step3_text_filtering():
    print("step3_text_filtering: start to change sentences pairs to features")
    pattern = {}

    with open(word_to_pattern, "r",encoding='utf-8') as f:
        line = f.readline()
        while line:
            temp = line.split()
            #	          print(temp)
            for t in temp:
                if t in pattern.keys():
                    pattern[t] += 1
                else:
                    pattern[t]=1
            line = f.readline()
        f.close()
    def cmp(a,b):
        if a > b:
            return 1
        elif a < b:
            return -1
        else :
            return 0

    #patterns = sorted(pattern, lambda x, y: cmp(x[1], y[1]))#将pattern按照出现的次数降序排序
    patterns = pattern
    # pos = 1
    # for k in patterns.keys():
    #     patterns[k] = pos
    #     pos += 1
    pos = 0
    print(len(patterns))
    for k, v in list(patterns.items()):
        if patterns[k] <= 1: patterns.pop(k)
        else:
            patterns[k] = pos
            pos += 1

    print(pos)
    print(len(patterns))

    with open(final_pattern,'wb') as f:
        pickle.dump(pattern, f)
        f.close()


    with open(word_to_pattern,"r",encoding ='utf-8')as f1,open(pairs_features,'w',encoding='utf-8') as f3:
        line1 = f1.readline().strip().split()
        f3.write(str(pos) + "\n" )
        while line1:
            # print(line1)
            # print(line2)
            s = ""
            for t in line1:
                if t in patterns.keys():
                    s +=" "+str(patterns[t])+":"+str(1)
            if s  is "":
                s = "0:1"
            f3.write(s+"\n" )
            # print(s)
            # print(inf)
            #	          print(line1)
            #	          print(line2)
            #	          print(s+"\n")
            line1 = f1.readline().strip().split()
        f1.close()
        f3.close()
    print("step3 end")


from scipy.sparse import csr_matrix

# csr_matrix
# (data, (row_ind, col_ind)

def step4_lr():
    print("step4_lr: start to change sentences pairs to features")
    # data = pd.DataFrame()
    features_data = []
    row_ind = []
    col_ind = []

    with open(pairs_features,'r') as f:
        sizes_of_features = int(f.readline())
        print(sizes_of_features)
        line = f.readline().strip()
        # print(line)
        line_count = 0
        while line:
            # temp = pd.Series([0]* length)
            for k in line.split():
                #temp[int(k.split(':')[0])] =1
                temp = int(k.split(':')[0])
                if temp not in col_ind:
                    row_ind.append(line_count)
                    col_ind.append(temp)
                    features_data.append(1)
            line_count += 1
            print(line_count)
            line = f.readline().strip()
            # print(row_ind)
            # print(col_ind)
            # print(features_data)
        # print("line_count:"+ str(line_count))
    # print("len(col_ind):"+str(len(col_ind)))
    f.close()

    data = csr_matrix((features_data, (row_ind,col_ind)),shape=(Length,sizes_of_features))
    data_y = pd.read_csv(data_path).pop('res')
    data_y = data_y[0:M]

    print(data.shape)

    from sklearn.model_selection import train_test_split
    from sklearn.metrics import f1_score
    from sklearn.linear_model import LogisticRegression
    from sklearn.svm import SVC


    for step in range(10):
        cls = LogisticRegression()

        x_train, x_test, y_train, y_test = train_test_split(data, data_y, test_size = 0.3)
        # 选择模型
        # 把数据交给模型训练

        cls.fit(x_train, y_train)
        LogisticRegression_y_pred = cls.predict(x_test)


        # dataset_dense = x_train.todense()
        # print(dataset_dense[1][0:50])
        # print(y_train[0:20])
        # dataset_dense_test = x_test.todense()
        # print(dataset_dense_test[1][0:50])
        # print(y_test[0:20])
        # print(y_test)
        print(LogisticRegression_y_pred)
        print("LogisticRegression_y_pred step :%d  F1_score: %.10f" %(step ,f1_score(y_test, LogisticRegression_y_pred)))
        # f = open(result_path,'w+',encoding = 'utf-8')
        #
        # f.write( "step :%d  F1_score: %.10f" %(step,f1_score(y_test, LogisticRegression_y_pred)))
        #
        clf = SVC(gamma='auto')
        clf.fit(x_train, y_train)
        svm_y_pred = clf.predict(x_test)
        print("svm_y_pred step :%d  F1_score: %.10f" % (step, f1_score(y_test, svm_y_pred)))

    s = pickle.dumps(cls)
    f = open(model_path, 'wb')
    f.write(s)
    f.close()



if __name__ == '__main__':
    step1_cutword_to_pattern()
    step2_cut_top_word()
    step3_text_filtering()
    step4_lr()

