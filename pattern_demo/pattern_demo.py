import re
import jieba as jb
import os
import shutil
import pickle
import pandas as pd
from sklearn.externals import joblib

"""
   rm the characters like ,/; except chinese and numbers . cut the sentences to words,and pair the to pattern
                each lines of file is contains two sentences,and a tag 0(means two sentences have different  meaning) or 1( same meaning). 
    setence_word_q1/setence_word_q2: use the Chinese Word Segmentation API jieba to cut the q1/q2 sentences to words,and save it in file  {setence_word_q1/setence_word_q2} 
    word_to_pattern : pair one word of q1 and one word of q2 to pattern,and save all the pattern in file{word_to_pattern}
"""

root_path = r'E:\study\hrg_project\environment\dataset\precision_data'

data_path = root_path + r'/train.csv'
word_dict = root_path + r'/Chinese_stopwords.txt'

sentence_word_q1 = root_path + r'/dataset/train_15_word_q1.txt'
sentence_word_q2 = root_path + r'/dataset/train_15_word_q2.txt'
word_to_pattern = root_path + r'/dataset/word_to_pattern.txt'
patten_path = root_path + r'/dataset/word_to_pattern.txt'
final_pattern = root_path + r'/dataset/final_pattern.txt'
pairs_features = root_path + r'/dataset/pairs_features.txt'
result_path = root_path + r'/result.txt'
model_path = root_path + r'/model.txt'
# M = 100000

dataset = pd.read_csv(data_path, encoding = "GB2312")
# data = dataset[0:M]
data = dataset
Length, Width = data.shape


def step1_cutword_to_pattern():
	print("step1_cutword_to_pattern :start")

	#非中文和数字字符 \u0030-\u0039 数字0-9  \u4e00-\u9fa5 所有中文字符
	pattern = re.compile(r'[^\u4e00-\u9fa5]')

	#将所有的q1，去除非中文字符，并且进行结巴分词，之后一行一个句子的方式保存

	print("start cutting setence one")
	with open(sentence_word_q1, "w",encoding='utf-8') as f:
		i = 0;
		while i < Length:
			q1= re.sub(pattern, "",data['q1'][i])
			print(q1)
			temp = jb.cut(q1, cut_all=False)
			f.write(" ".join(temp) + "\n")
			i += 1
		f.close()
	print("cutting setences one has finished")

	#将所有的q2，去除非中文字符，并且进行结巴分词，之后一行一个句子的方式保存
	print("start cutting setence two")
	with open(sentence_word_q2, "w",encoding='utf-8') as f:
		i = 0;
		while i < Length:
			q2= re.sub(pattern, "",data['q2'][i])
			print(q2)
			temp = jb.cut(q2, cut_all=False)
			f.write(" ".join(temp)+ "\n")
			i += 1
		f.close()
	print("cutting setences two has finished")
	print("step1 end")

def rm_stopwords(file_path, word_dict):
	"""
	    rm stop word for {file_path}, stop words save in {word_dict} file.
	    file_path: file path of file generated by function splitwords.
	                each lines of file is contains many words ,each word separate from each other by space.
	    word_dict: file containing stop words, and every stop words in one line.
	    output: file_path which have been removed stop words and overwrite original file.
	"""

	# 从文件中将停用词读取出来，并保存到字典stop_dict中
	stop_dict = {}
	with open(word_dict,'r', encoding='UTF-8') as d:
		for word in d:
			stop_dict[word.strip("\n")] = 1
	# 如果存在临时文件将其删除
	if os.path.exists(file_path + ".tmp"):
		os.remove(file_path + ".tmp")

	print ("now remove stop words in %s." % file_path)
	# 安行删除停用词
	with open(file_path,'r', encoding='UTF-8') as f1, open(file_path + ".tmp", "w", encoding='UTF-8') as f2:
		for line in f1:
			tmp_list = []  # 将非停用词保存下来
			words = line.split()
			#	        print(line)
			for word in words:
				if word not in stop_dict:
					tmp_list.append(word)
			words_without_stop = " ".join(tmp_list)
			to_write = words_without_stop + "\n"
			f2.write(to_write)
		f1.close()
		f2.close()
	# 将去除停用词的pattern文件重新到原路径
	shutil.move(file_path + ".tmp", file_path)
	print ("stop words in %s has been removed." % file_path)



def step2_cut_top_word():
	print ("step2_cut_top_word: now combine two words ,one from setence one and another from setence two. and save in %s." %word_to_pattern)
	#q1,q2 分词之后，分别两两组合，形成pattern
	with open(sentence_word_q1,"r",encoding ='utf-8')as f1,open(sentence_word_q2,'r',encoding = 'utf-8') as f2,open(word_to_pattern,'w',encoding='utf-8') as f3:
		line1 = f1.readline()
		line2 = f2.readline()

		while line1 and line2:
			temp=""
			for t in line1.split():
				for k in line2.split():
					temp = temp+" "+t+k
			f3.write(temp+'\n')
			# print(temp)
			line1 = f1.readline()
			line2 = f2.readline()
		f1.close()
		f2.close()
		f3.close()
	rm_stopwords(word_to_pattern, word_dict)
	print("step end. combine  words to pattern has  finished")


def step3_text_filtering():
	print("step3_text_filtering: start to change sentences pairs to features")
	pattern = {}
	with open(patten_path, "r",encoding='utf-8') as f:
		line = f.readline();
		while line:
			temp = line.split()
			#	          print(temp)
			for t in temp:
				if t in pattern.keys():
					pattern[t] += 1
				else:
					pattern[t]=1
			line = f.readline()
		f.close()
	def cmp(a,b):
		if a > b:
			return 1
		elif a < b:
			return -1
		else :
			return 0

	#patterns = sorted(pattern, lambda x, y: cmp(x[1], y[1]))#将pattern按照出现的次数降序排序
	patterns = pattern
	# pos = 1
	# for k in patterns.keys():
	#     patterns[k] = pos
	#     pos += 1
	pos = 0
	for k, v in list(patterns.items()):
		if patterns[k] <= 1: patterns.pop(k)
		else:
			patterns[k] = pos
			pos += 1
	#	print(pos)

	with open(final_pattern,'wb') as f:
		pickle.dump(pattern, f)
		f.close()


	with open(sentence_word_q1,"r",encoding ='utf-8')as f1,open(sentence_word_q2,'r',encoding = 'utf-8') as f2,open(pairs_features,'w',encoding='utf-8') as f3:
		line1 = f1.readline()
		line2 = f2.readline()
		f3.write(str(pos) + "\n" )
		while line1 and line2:
			s = ""
			for t in line1:
				for k in line2:
					if t+k in patterns.keys():
						s +=" "+str(patterns[t+k])+":"+str(1)

			f3.write(s+"\n")
			#	          print(line1)
			#	          print(line2)
			#	          print(s+"\n")
			line1 = f1.readline()
			line2 = f2.readline()
		f1.close()
		f2.close()
		f3.close()
	print("step3 end")


from scipy.sparse import csr_matrix

# csr_matrix
# (data, (row_ind, col_ind)

def step4_lr():
	print("step4_lr: start to change sentences pairs to features")
	# data = pd.DataFrame()
	features_data = []
	row_ind = []
	col_ind = []

	with open(pairs_features,'r',encoding = 'utf-8') as f:
		length = int(f.readline())
		line = f.readline()
		line_count = 0
		while line:
			# temp = pd.Series([0]* length)
			for k in line.split():
				#temp[int(k.split(':')[0])] =1
				row_ind.append(line_count)
				col_ind.append(int(k.split(':')[0]))
				features_data.append(1)
		# data = data.append(temp,ignore_index=True)
			line = f.readline()
			line_count += 1
	f.close()

	data = csr_matrix((features_data, (row_ind,col_ind)),shape=(Length,length))
	#csr_matrix((data, (row_ind, col_ind)), [shape = (M, N)])
	data_y = pd.read_csv(data_path,encoding = "GB2312").pop('res')
	# data_y = data_y[0:M]

	print(data.shape)

	from sklearn.model_selection import train_test_split
	from sklearn.metrics import f1_score
	from sklearn.linear_model import LogisticRegression
	cls = LogisticRegression()
	for step in range(10):
		x_train, x_test, y_train, y_test = train_test_split(data, data_y, test_size = 0.3)
		# 选择模型
		# 把数据交给模型训练
		cls.fit(x_train, y_train)
		LogisticRegression_y_pred = cls.predict(x_test)
		print("step :%d  F1_score: %.10f" %(step ,f1_score(y_test, LogisticRegression_y_pred)))
		f = open(result_path,'w+',encoding = 'utf-8')
		f.write( "step :%d  F1_score: %.10f" %(step,f1_score(y_test, LogisticRegression_y_pred)))

	s = pickle.dumps(cls)
	f = open(model_path, 'wb')
	f.write(s)
	f.close()



if __name__ == '__main__':
	step1_cutword_to_pattern()
	step2_cut_top_word()
	step3_text_filtering()
	step4_lr()

