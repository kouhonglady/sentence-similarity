import re
import jieba as jb
import os
import shutil
import pickle
import pandas as pd

"""
   rm the characters like ,/; except chinese and numbers . cut the sentences to words,and pair the to pattern
                each lines of file is contains two sentences,and a tag 0(means two sentences have different  meaning) or 1( same meaning). 
    setence_word_q1/setence_word_q2: use the Chinese Word Segmentation API jieba to cut the q1/q2 sentences to words,and save it in file  {setence_word_q1/setence_word_q2} 
    word_to_pattern : pair one word of q1 and one word of q2 to pattern,and save all the pattern in file{word_to_pattern}
"""


root_path = r'E:\study\hrg_project\environment\dataset\precision_data'

data_path = root_path + r'/original/train_weizhong.csv'
word_dict = root_path + r'/original/Chinese_stopwords.txt'

sentence_word_q1 = root_path + r'/dataset/train_15_word_q1.txt'
sentence_word_q2 = root_path + r'/dataset/train_15_word_q2.txt'
word_to_pattern = root_path + r'/dataset/word_to_pattern.txt'
final_pattern = root_path + r'/dataset/final_pattern.txt'
pairs_features = root_path + r'/dataset/pairs_features.txt'
result_path = root_path + r'/result.txt'
model_path = root_path + r'/model_weizhong.txt'

features_data_path = root_path + r'\dataset\temp_data\features_data.csv'
row_ind_path = root_path + r'\dataset\temp_data\row_ind.csv'
col_ind_path = root_path + r'\dataset\temp_data\col_ind.csv'
data_y_path = root_path + r'\dataset\temp_data\data_y.csv'

data_path = r'./data/train2.csv'
sentence_word_q1 = r'./data/train_15_word_q1.txt'
sentence_word_q2 = r'./data/train_15_word_q2.txt'
word_to_pattern = r'./data/word_to_pattern.txt'
word_dict = r'./data/Chinese_stopwords.txt'
patten_path = r'./data/word_to_pattern.txt'
final_pattern = r'./data/final_pattern.txt'
pairs_features = r'./data/pairs_features.txt'
feature_path = r'./data/pairs_features.txt'


def step1_cutword_to_pattern():
    data = pd.read_csv(data_path)
    length, width = data.shape

    # 非中文和数字字符 \u0030-\u0039 数字0-9  \u4e00-\u9fa5 所有中文字符
    pattern = re.compile(r'[^\u0030-\u0039\u4e00-\u9fa5]')

    # 将所有的q1，去除非中文字符，并且进行结巴分词，之后一行一个句子的方式保存

    print("start cutting setence one")
    with open(sentence_word_q1, "w", encoding='utf-8') as f:
        i = 0;
        while i < length:
            q1 = re.sub(pattern, "", data['q1'][i])
            temp = jb.cut(q1, cut_all=False)
            f.write(" ".join(temp) + "\n")
            i += 1
        f.close()
    print("cutting setences one has finished")

    # 将所有的q2，去除非中文字符，并且进行结巴分词，之后一行一个句子的方式保存
    print("start cutting setence two")
    with open(sentence_word_q2, "w", encoding='utf-8') as f:
        i = 0;
        while i < length:
            q2 = re.sub(pattern, "", data['q2'][i])
            temp = jb.cut(q2, cut_all=False)
            f.write(" ".join(temp) + "\n")
            i += 1
        f.close()
    print("cutting setences two has finished")


def rm_stopwords(file_path, word_dict):
    """
        rm stop word for {file_path}, stop words save in {word_dict} file.
        file_path: file path of file generated by function splitwords.
                    each lines of file is contains many words ,each word separate from each other by space.
        word_dict: file containing stop words, and every stop words in one line.
        output: file_path which have been removed stop words and overwrite original file.
    """

    # 从文件中将停用词读取出来，并保存到字典stop_dict中
    stop_dict = {}
    with open(word_dict, 'r', encoding='UTF-8') as d:
        for word in d:
            stop_dict[word.strip("\n")] = 1
    # 如果存在临时文件将其删除
    if os.path.exists(file_path + ".tmp"):
        os.remove(file_path + ".tmp")

    print("now remove stop words in %s." % file_path)
    # 安行删除停用词
    with open(file_path, 'r', encoding='UTF-8') as f1, open(file_path + ".tmp", "w", encoding='UTF-8') as f2:
        for line in f1:
            tmp_list = []  # 将非停用词保存下来
            words = line.split()
            #	        print(line)
            for word in words:
                if word not in stop_dict:
                    tmp_list.append(word)
            words_without_stop = " ".join(tmp_list)
            to_write = words_without_stop + "\n"
            f2.write(to_write)
        f1.close()
        f2.close()
    # 将去除停用词的pattern文件重新到原路径
    shutil.move(file_path + ".tmp", file_path)
    print("stop words in %s has been removed." % file_path)


def step2_cut_top_word():
    rm_stopwords(sentence_word_q1, word_dict)

    rm_stopwords(sentence_word_q2, word_dict)

    print("now combine two words ,one from setence one and another from setence two. and save in %s." % word_to_pattern)
    # q1,q2 分词之后，分别两两组合，形成pattern
    with open(sentence_word_q1, "r", encoding='utf-8')as f1, open(sentence_word_q2, 'r', encoding='utf-8') as f2, open(
            word_to_pattern, 'w', encoding='utf-8') as f3:
        line1 = f1.readline()
        line2 = f2.readline()

        while line1 and line2:
            temp = ""
            for t in line1.split():
                for k in line2.split():
                    temp = temp + " " + t + k
            f3.write(temp + '\n')
            line1 = f1.readline()
            line2 = f2.readline()
        f1.close()
        f2.close()
        f3.close()
    print("combine  words to pattern has  finished")


def step3_text_filtering():
    pattern = {}
    with open(word_to_pattern, "r", encoding='utf-8') as f:
        line = f.readline();
        while line:
            temp = line.split()
            #	          print(temp)
            for t in temp:
                if t in pattern.keys():
                    pattern[t] += 1
                else:
                    pattern[t] = 1
            line = f.readline()
        f.close()

    def cmp(a, b):
        if a > b:
            return 1
        elif a < b:
            return -1
        else:
            return 0

    # patterns = sorted(pattern, lambda x, y: cmp(x[1], y[1]))#将pattern按照出现的次数降序排序
    patterns = pattern
    # pos = 1
    # for k in patterns.keys():
    #     patterns[k] = pos
    #     pos += 1
    pos = 0
    for k, v in list(patterns.items()):
        if patterns[k] == 1:
            patterns.pop(k)
        else:
            patterns[k] = pos
            pos += 1
    #	print(pos)

    with open(final_pattern, 'wb') as f:
        pickle.dump(pattern, f)
        f.close()

    with open(sentence_word_q1, "r", encoding='utf-8')as f1, open(sentence_word_q2, 'r', encoding='utf-8') as f2, open(
            pairs_features, 'w', encoding='utf-8') as f3:
        line1 = f1.readline()
        line2 = f2.readline()
        f3.write(str(pos) + "\n")
        while line1 and line2:
            s = ""
            for t in line1:
                for k in line2:
                    if t + k in patterns.keys():
                        s += " " + str(patterns[t + k]) + ":" + str(1)

            f3.write(s + "\n")
            #	          print(line1)
            #	          print(line2)
            #	          print(s+"\n")
            line1 = f1.readline()
            line2 = f2.readline()

        f1.close()
        f2.close()
        f3.close()


def step4_lr():
    data = pd.DataFrame()
    with open(pairs_features, 'r', encoding='utf-8') as f:
        length = int(f.readline())
        line = f.readline()
        while line:
            temp = pd.Series([0] * length)
            for k in line.split():
                temp[int(k.split(':')[0])] = 1
            data = data.append(temp, ignore_index=True)
            line = f.readline()
        f.close()

    data_y = pd.read_csv(data_path).pop('res')

    from sklearn.cross_validation import train_test_split
    x_train, x_test, y_train, y_test = train_test_split(data, data_y, test_size=0.3)

    from sklearn.linear_model import LinearRegression
    reg = LinearRegression()
    reg.fit(x_train, y_train)
    reg.coef_
    LinearRegression_y_pred = reg.predict(x_test)
    s = pickle.dumps(reg)
    f = open('reg.txt', 'wb')
    f.write(s)
    f.close()

    # LinearRegression_y_pred2 = reg2.predict(x_test)

    from sklearn.metrics import mean_squared_error, r2_score
    print("Mean squared error: %.10f"
          % mean_squared_error(y_test, LinearRegression_y_pred))
    # Explained variance score: 1 is perfect prediction
    print('Variance score: %.10f' % r2_score(y_test, LinearRegression_y_pred))

    from sklearn.linear_model import LogisticRegression

    # 选择模型
    cls = LogisticRegression()
    # 把数据交给模型训练
    cls.fit(x_train, y_train)
    LogisticRegression_y_pred = cls.predict(x_test)

    s = pickle.dumps(cls)
    f = open('cls.txt', 'wb')
    f.write(s)
    f.close()

    # print("Coefficients:%s, intercept %s"%(cls.coef_,cls.intercept_))
    # print("Residual sum of squares: %.2f"% np.mean((cls.predict(x_test) - y_test) ** 2))
    # print('Score: %.2f' % cls.score(x_test, y_test))
    print("Mean squared error: %.10f"
          % mean_squared_error(y_test, LogisticRegression_y_pred))
    # Explained variance score: 1 is perfect prediction
    print('Variance score: %.10f' % r2_score(y_test, LogisticRegression_y_pred))
    with open("res.txt", "w") as f:
        f.write("1 Mean squared error: %.10f\n"
                % mean_squared_error(y_test, LinearRegression_y_pred))
        f.write('Variance score: %.10f\n' % r2_score(y_test, LinearRegression_y_pred))
        f.write("2 Mean squared error: %.10f\n"
                % mean_squared_error(y_test, LogisticRegression_y_pred))
        f.write('Variance score: %.10f\n' % r2_score(y_test, LogisticRegression_y_pred))
        f.close()


if __name__ == '__main__':
    step1_cutword_to_pattern()
    step2_cut_top_word()
    step3_text_filtering()
    step4_lr()